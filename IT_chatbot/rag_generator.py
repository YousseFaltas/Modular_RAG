from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationSummaryBufferMemory
import os
from dotenv import load_dotenv
import re
from typing import Dict, Tuple
from langdetect import detect
import time
import threading

from helpers.date_agent import DateAgent
from helpers.langsmith_config import setup_langsmith
from helpers.retrieval import get_rag_context

load_dotenv()

# -- date agent --
date_agent = DateAgent(timezone=os.getenv("DEFAULT_TIMEZONE", "Africa/Cairo"))

# -- LangSmith setup --
setup_langsmith()

# -- LLM and prompt setup --
openai_api_key = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(
    model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
    temperature=float(os.getenv("OPENAI_TEMPERATURE", "0.5")),
    openai_api_key=openai_api_key
)

# =============================================================================
# Memory Management with TTL Eviction
# =============================================================================

# Configuration
MAX_TOKEN_LIMIT = 500
CHAIN_TTL_SECONDS = int(os.getenv("CHAIN_TTL_SECONDS", 3600))  # Default 1 hour
MAX_CHAINS = int(os.getenv("MAX_CHAINS", 1000))  # Maximum number of chains to store
CLEANUP_INTERVAL_SECONDS = 300  # Run cleanup every 5 minutes

# In-memory store for user chat chains with timestamps
# Format: {user_id: (chain, last_access_timestamp, language)}
chat_chains: Dict[str, Tuple[LLMChain, float, str]] = {}
chains_lock = threading.Lock()


def cleanup_expired_chains():
    """Remove expired chains based on TTL and enforce max chain limit."""
    current_time = time.time()
    with chains_lock:
        # Remove expired chains
        expired_users = [
            user_id for user_id, (_, last_access, _) in chat_chains.items()
            if current_time - last_access > CHAIN_TTL_SECONDS
        ]
        for user_id in expired_users:
            del chat_chains[user_id]
        
        if expired_users:
            print(f"ðŸ§¹ Cleaned up {len(expired_users)} expired chat chains")
        
        # If still over limit, remove oldest chains (LRU eviction)
        if len(chat_chains) > MAX_CHAINS:
            sorted_by_access = sorted(
                chat_chains.items(),
                key=lambda x: x[1][1]  # Sort by last_access timestamp
            )
            to_remove = len(chat_chains) - MAX_CHAINS
            for user_id, _ in sorted_by_access[:to_remove]:
                del chat_chains[user_id]
            print(f"ðŸ§¹ LRU evicted {to_remove} chat chains (over limit)")


def start_cleanup_scheduler():
    """Start a background thread for periodic cleanup."""
    def run_cleanup():
        while True:
            time.sleep(CLEANUP_INTERVAL_SECONDS)
            try:
                cleanup_expired_chains()
            except Exception as e:
                print(f"Error during cleanup: {e}")
    
    cleanup_thread = threading.Thread(target=run_cleanup, daemon=True)
    cleanup_thread.start()


# Start the cleanup scheduler when module is loaded
start_cleanup_scheduler()


# =============================================================================
# Prompt Templates
# =============================================================================

english_template_str = """
Identity:
 - You are the AI assistant for Beltone Holding.
 - Beltone a leading financial services provider in the MENA region, your purpose is to showcase our commitment to redefining the regional financial ecosystem through innovative, value-driven solutions.
 - Beltone is a holding company that has many subsidiaries and does not have branches.

Mission:
 - Your main goal is to provide clear, concise, and engaging information about Beltone.
 - You should embody a tone that is professional yet warm and encouraging.
 - When answering questions, your communication style should be conversational and simple.
 - Feel free to use contractions and simplify complex topics into easy-to-understand concepts.
 - Use markdown to structure your answers clearly and cleanly.
Rules:
 - ANSWER IN ENGLISH DESPITE OF THE HISTORY OR THE CONTEXT LANGUAGE.
 - Context is Key: Always base your answers on the provided context. Do not give information not mentioned in the context.Do not mention that you are using context to answer. Rephrase the context as you see fit.
 - No Hallucinations: Do not invent information. If you cannot answer a question based on the context, state that you do not have the information to help with that specific query.
 - Greetings: Only greet a user if they greet you first. If they do, greet them back and briefly introduce yourself as Beltone's assistant.
 - Relevance: You must avoid answering questions that are unrelated to Beltone or its services.
 - Originality: Never give the exact same answer twice.
 - Conciseness: Do not exceed 1000 characters in your response.


Context:
{context}


{history}


Question:
{input}
"""
english_prompt = PromptTemplate(input_variables=["history", "input", "context"], template=english_template_str)

arabic_template_str = """
Ø§Ù„Ù‡ÙˆÙŠØ©:
- Ø£Ù†Øª Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ (AI assistant) Ù„Ø´Ø±ÙƒØ© Ø¨Ù„ØªÙˆÙ† Ø§Ù„Ù‚Ø§Ø¨Ø¶Ø© (Beltone Holding).
- Ø¨Ù„ØªÙˆÙ† Ù‡ÙŠ Ø´Ø±ÙƒØ© Ø±Ø§Ø¦Ø¯Ø© ÙÙŠ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ø§Ù„ÙŠØ© ÙÙŠ Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ Ø§Ù„Ø£ÙˆØ³Ø· ÙˆØ´Ù…Ø§Ù„ Ø£ÙØ±ÙŠÙ‚ÙŠØ§ØŒ ÙˆÙ‡Ø¯ÙÙƒ Ù‡Ùˆ Ø¥Ø¨Ø±Ø§Ø² Ø§Ù„ØªØ²Ø§Ù…Ù†Ø§ Ø¨Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¥Ù‚Ù„ÙŠÙ…ÙŠ Ù…Ù† Ø®Ù„Ø§Ù„ Ø­Ù„ÙˆÙ„ Ù…Ø¨ØªÙƒØ±Ø© ØªÙ‚Ø¯Ù… Ù‚ÙŠÙ…Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ©.
- Ø¨Ù„ØªÙˆÙ† Ù‡ÙŠ Ø´Ø±ÙƒØ© Ù‚Ø§Ø¨Ø¶Ø© Ù„Ø¯ÙŠÙ‡Ø§ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø´Ø±ÙƒØ§Øª Ø§Ù„ØªØ§Ø¨Ø¹Ø© ÙˆÙ„ÙŠØ³ Ù„Ø¯ÙŠÙ‡Ø§ ÙØ±ÙˆØ¹.

Ø§Ù„Ù…Ù‡Ù…Ø©:
- Ù‡Ø¯ÙÙƒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù‡Ùˆ ØªÙ‚Ø¯ÙŠÙ… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…ÙˆØ¬Ø²Ø© ÙˆØ¬Ø°Ø§Ø¨Ø© Ø¹Ù† Ø¨Ù„ØªÙˆÙ†.
- ÙŠØ¬Ø¨ Ø£Ù† ØªØªØ­Ù„Ù‰ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù…Ù‡Ù†ÙŠ ÙˆÙ„ÙƒÙ† ÙˆØ¯ÙˆØ¯ ÙˆÙ…Ø´Ø¬Ø¹.
- Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©ØŒ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø­Ø§Ø¯Ø«Ø§ØªÙŠ ÙˆØ¨Ø³ÙŠØ·.
- Ù„Ø§ ØªØªØ±Ø¯Ø¯ ÙÙŠ ØªØ¨Ø³ÙŠØ· Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© Ø¥Ù„Ù‰ Ù…ÙØ§Ù‡ÙŠÙ… Ø³Ù‡Ù„Ø© Ø§Ù„ÙÙ‡Ù….
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ (markdown) Ù„ØªÙ†Ø¸ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ Ø¨ÙˆØ¶ÙˆØ­.

Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯:
- Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨ØºØ¶ Ø§Ù„Ù†Ø¸Ø± Ø¹Ù† Ù„ØºØ© Ø§Ù„Ø³ÙŠØ§Ù‚ Ø£Ùˆ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©.
- Ø§Ù„Ø³ÙŠØ§Ù‚ Ù‡Ùˆ Ø§Ù„Ø£Ø³Ø§Ø³: Ø§Ø¨Ù†ÙŠ Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù…. Ù„Ø§ ØªØ¹Ø·ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…Ø°ÙƒÙˆØ±Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚. Ù„Ø§ ØªØ°ÙƒØ± Ø£Ù†Ùƒ ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø©. Ø£Ø¹Ø¯ ØµÙŠØ§ØºØ© Ø§Ù„Ø³ÙŠØ§Ù‚ ÙƒÙ…Ø§ ØªØ±Ø§Ù‡ Ù…Ù†Ø§Ø³Ø¨Ø§Ù‹.
- Ù„Ø§ ØªØ®ØªÙ„Ù‚ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª: Ù„Ø§ ØªØ®ØªØ±Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª. Ø¥Ø°Ø§ Ù„Ù… ØªØ³ØªØ·Ø¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ø§Ø°ÙƒØ± Ø£Ù†Ù‡ Ù„ÙŠØ³ Ù„Ø¯ÙŠÙƒ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø± Ø§Ù„Ù…Ø­Ø¯Ø¯.
- Ø§Ù„ØªØ­ÙŠØ§Øª: Ø±Ø¯ Ø§Ù„ØªØ­ÙŠØ© ÙÙ‚Ø· Ø¥Ø°Ø§ Ø¨Ø¯Ø£ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„ØªØ­ÙŠØ©. Ø¥Ø°Ø§ ÙØ¹Ù„ Ø°Ù„ÙƒØŒ Ø±Ø¯ Ø§Ù„ØªØ­ÙŠØ© ÙˆÙ‚Ø¯Ù… Ù†ÙØ³Ùƒ Ø¨Ø¥ÙŠØ¬Ø§Ø² ÙƒÙ…Ø³Ø§Ø¹Ø¯ Ø¨Ù„ØªÙˆÙ†.
- Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹: ÙŠØ¬Ø¨ Ø£Ù† ØªØªØ¬Ù†Ø¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ØªÙŠ Ù„Ø§ ØªØªØ¹Ù„Ù‚ Ø¨Ø¨Ù„ØªÙˆÙ† Ø£Ùˆ Ø®Ø¯Ù…Ø§ØªÙ‡Ø§.
- Ø§Ù„Ø£ØµØ§Ù„Ø©: Ù„Ø§ ØªØ¹Ø·ÙŠ Ù†ÙØ³ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø±ØªÙŠÙ†.
- Ø§Ù„Ø¥ÙŠØ¬Ø§Ø²: Ù„Ø§ ØªØªØ¬Ø§ÙˆØ² 1000 Ø­Ø±Ù ÙÙŠ Ø¥Ø¬Ø§Ø¨ØªÙƒ.


Ø§Ù„Ø³ÙŠØ§Ù‚:
{context}


{history}


Ø§Ù„Ø³Ø¤Ø§Ù„:
{input}
"""
arabic_prompt = PromptTemplate(input_variables=["history", "input", "context"], template=arabic_template_str)


# =============================================================================
# Core Functions
# =============================================================================

def get_or_create_conversation_chain(user_id: str, lang: str) -> LLMChain:
    """
    Get an existing conversation chain or create a new one.
    
    Implements language switching: if user switches language, recreate the chain
    with the appropriate prompt template.
    """
    current_time = time.time()
    
    with chains_lock:
        if user_id in chat_chains:
            chain, last_access, stored_lang = chat_chains[user_id]
            
            # Check if language changed - if so, create new chain
            if stored_lang != lang:
                print(f"ðŸ”„ Language switch detected for user {user_id}: {stored_lang} -> {lang}")
                # Remove old chain to create new one with correct language
                del chat_chains[user_id]
            else:
                # Update access time and return existing chain
                chat_chains[user_id] = (chain, current_time, stored_lang)
                return chain
        
        # Create new chain
        memory = ConversationSummaryBufferMemory(
            llm=llm,
            max_token_limit=MAX_TOKEN_LIMIT,
            memory_key="history",
            input_key="input",
            return_messages=False
        )
        prompt_template = arabic_prompt if lang == "ar" else english_prompt
        new_chain = LLMChain(
            llm=llm,
            memory=memory,
            prompt=prompt_template,
            verbose=False
        )
        chat_chains[user_id] = (new_chain, current_time, lang)
        return new_chain


def detect_language(text):
    try:
        return "ar" if detect(text) == 'ar' else "en"
    except Exception:
        return "en"


def classify_question_type(question: str, history: str, llm) -> str:
    prompt = f"""
    Given the following conversation history:
    {history}
    And this new user question:
    {question}
    Determine if the new question is:
    - A 'follow-up' (refers to or depends on the previous conversation), or
    - A 'new question' (does not depend on previous context).

    Respond with only: 'follow-up' or 'new question'.
    """
    try:
        answer = llm.invoke(prompt).content.strip().lower()
        return answer
    except Exception:
        return "new question"


def clean_response(response: str) -> str:
    response = re.sub(r"<think>.*?</think>", "", response, flags=re.DOTALL)
    response = re.sub(r"[*_#]+", "", response)
    response = re.sub(r"\s+", " ", response).strip()
    response = re.sub(r"[{}]+", "", response)
    return response


def rag_answer_with_memory(question: str, user_id: str, top_k: int = 7) -> str:
    lang = detect_language(question)
    conversation = get_or_create_conversation_chain(user_id, lang)
    history = ""
    if hasattr(conversation.memory, "buffer"):
        history = conversation.memory.buffer

    question_type = classify_question_type(question, history, llm)
    search_query = question  # simple fallback
    try:
        search_query = conversation.predict(input=question, context="", history=history)
    except Exception:
        search_query = question

    rag_context = get_rag_context(search_query, lang, top_k)
    rag_context = date_agent.enhance_context_with_date(rag_context, question)

    injected_history = history if question_type == "follow-up" else ""

    try:
        response = conversation.predict(input=question, context=rag_context, history=injected_history)
        response_clean = clean_response(response)
        return response_clean
    except Exception as e:
        return f"âŒ An error occurred: {e}"


def get_chain_stats() -> Dict:
    """Get statistics about the chat chains for monitoring."""
    with chains_lock:
        current_time = time.time()
        return {
            "total_chains": len(chat_chains),
            "max_chains": MAX_CHAINS,
            "ttl_seconds": CHAIN_TTL_SECONDS,
            "oldest_chain_age": max(
                (current_time - last_access for _, (_, last_access, _) in chat_chains.items()),
                default=0
            )
        }
