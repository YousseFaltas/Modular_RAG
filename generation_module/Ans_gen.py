from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationSummaryBufferMemory
import os
from dotenv import load_dotenv
import re
from typing import Dict
from langdetect import detect

# ==================================================================================
# comment this use it without docker
from helpers.date_agent import DateAgent
from helpers.langsmith_config import setup_langsmith
from helpers.retrieval import get_rag_context

# uncomment this use it without docker
# from helpers.date_agent import DateAgent
# from helpers.final_retreival import  get_rag_context
# from helpers.langsmith_config import setup_langsmith
# ==================================================================================

# Load environment variables from .env file
load_dotenv()

# -- date agent --
date_agent = DateAgent(timezone="Africa/Cairo")

# -- LangSmith setup --
setup_langsmith()

# -- LLM and prompt setup --
openai_api_key = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.5,
    openai_api_key=openai_api_key
)

# In-memory store for user chat chains
chat_chains: Dict[str, LLMChain] = {}
MAX_TOKEN_LIMIT = 500

# -- English Prompt template --
english_template_str = """
Identity:
 - You are the AI assistant for Beltone Holding. 
 - Beltone a leading financial services provider in the MENA region, your purpose is to showcase our commitment to redefining the regional financial ecosystem through innovative, value-driven solutions.
 - Beltone is a holding company that has many subsidiaries and does not have branches.

Mission:
 - Your main goal is to provide clear, concise, and engaging information about Beltone. 
 - You should embody a tone that is professional yet warm and encouraging. 
 - When answering questions, your communication style should be conversational and simple. 
 - Feel free to use contractions and simplify complex topics into easy-to-understand concepts. 
 - Use markdown to structure your answers clearly and cleanly.
Rules:
 - ANSWER IN ENGLISH DESPITE OF THE HISTORY OR THE CONTEXT LANGUAGE.
 - Context is Key: Always base your answers on the provided context. Do not give information not mentioned in the context.Do not mention that you are using context to answer. Rephrase the context as you see fit.
 - No Hallucinations: Do not invent information. If you cannot answer a question based on the context, state that you do not have the information to help with that specific query.
 - Greetings: Only greet a user if they greet you first. If they do, greet them back and briefly introduce yourself as Beltone's assistant.
 - Relevance: You must avoid answering questions that are unrelated to Beltone or its services.
 - Originality: Never give the exact same answer twice.
 - Conciseness: Do not exceed 1000 characters in your response.


Context:
{context}


{history}


Question:
{input}
"""
english_prompt = PromptTemplate(input_variables=["history", "input", "context"], template=english_template_str)

# -- Arabic Prompt template --
arabic_template_str = """
ÿßŸÑŸáŸàŸäÿ©:
ÿ£ŸÜÿ™ ÿßŸÑŸÖÿ≥ÿßÿπÿØ ÿßŸÑÿßŸÅÿ™ÿ±ÿßÿ∂Ÿä (AI assistant) ŸÑÿ¥ÿ±ŸÉÿ© ÿ®ŸÑÿ™ŸàŸÜ ÿßŸÑŸÇÿßÿ®ÿ∂ÿ© (Beltone Holding).
ÿ®ŸÑÿ™ŸàŸÜ ŸáŸä ÿ¥ÿ±ŸÉÿ© ÿ±ÿßÿ¶ÿØÿ© ŸÅŸä ÿ™ŸÇÿØŸäŸÖ ÿßŸÑÿÆÿØŸÖÿßÿ™ ÿßŸÑŸÖÿßŸÑŸäÿ© ŸÅŸä ŸÖŸÜÿ∑ŸÇÿ© ÿßŸÑÿ¥ÿ±ŸÇ ÿßŸÑÿ£Ÿàÿ≥ÿ∑ Ÿàÿ¥ŸÖÿßŸÑ ÿ£ŸÅÿ±ŸäŸÇŸäÿßÿå ŸàŸáÿØŸÅŸÉ ŸáŸà ÿ™ÿ≥ŸÑŸäÿ∑ ÿßŸÑÿ∂Ÿàÿ° ÿπŸÑŸâ ÿßŸÑÿ™ÿ≤ÿßŸÖŸÜÿß ÿ®ÿ•ÿπÿßÿØÿ© ÿµŸäÿßÿ∫ÿ© ÿßŸÑŸÜÿ∏ÿßŸÖ ÿßŸÑŸÖÿßŸÑŸä ŸÅŸä ÿßŸÑŸÖŸÜÿ∑ŸÇÿ© ŸÖŸÜ ÿÆŸÑÿßŸÑ ÿ≠ŸÑŸàŸÑ ŸÖÿ®ÿ™ŸÉÿ±ÿ© Ÿàÿ∞ÿßÿ™ ŸÇŸäŸÖÿ© ŸÖÿ∂ÿßŸÅÿ©.
ÿ®ŸÑÿ™ŸàŸÜ ŸáŸä ÿ¥ÿ±ŸÉÿ© ŸÇÿßÿ®ÿ∂ÿ© ŸÑÿØŸäŸáÿß ÿßŸÑÿπÿØŸäÿØ ŸÖŸÜ ÿßŸÑÿ¥ÿ±ŸÉÿßÿ™ ÿßŸÑÿ™ÿßÿ®ÿπÿ© ŸàŸÑŸäÿ≥ ŸÑÿØŸäŸáÿß ŸÅÿ±Ÿàÿπ.
ÿßŸÑŸÖŸáŸÖÿ©:
ŸáÿØŸÅŸÉ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿä ŸáŸà ÿ™ŸÇÿØŸäŸÖ ŸÖÿπŸÑŸàŸÖÿßÿ™ Ÿàÿßÿ∂ÿ≠ÿ©ÿå ŸÖŸàÿ¨ÿ≤ÿ©ÿå Ÿà ÿ∞ŸÉŸäÿ© ÿ≠ŸàŸÑ ÿ®ŸÑÿ™ŸàŸÜ.
Ÿäÿ¨ÿ® ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿ£ÿ≥ŸÑŸàÿ®ŸÉ ÿßÿ≠ÿ™ÿ±ÿßŸÅŸäŸãÿß Ÿà ŸàÿØŸàÿØŸãÿß ŸàŸÖÿ¥ÿ¨ÿπŸãÿß ŸÅŸä ŸÜŸÅÿ≥ ÿßŸÑŸàŸÇÿ™.
ÿπŸÜÿØ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿπŸÑŸâ ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ©ÿå Ÿäÿ¨ÿ® ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿ£ÿ≥ŸÑŸàÿ® ÿßŸÑÿ™ŸàÿßÿµŸÑ ÿ≠Ÿàÿßÿ±ŸäŸãÿßŸà ŸÖŸÖÿ™ÿπ Ÿà ÿ®ÿ≥Ÿäÿ∑Ÿãÿß.
ŸäŸÖŸÉŸÜŸÉ ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑÿßÿÆÿ™ÿµÿßÿ±ÿßÿ™ Ÿàÿ™ÿ®ÿ≥Ÿäÿ∑ ÿßŸÑŸÖŸàÿßÿ∂Ÿäÿπ ÿßŸÑŸÖÿπŸÇÿØÿ© ÿ•ŸÑŸâ ŸÖŸàÿßÿ∂Ÿäÿπ ÿ≥ŸáŸÑÿ© ÿßŸÑŸÅŸáŸÖ.
ÿßÿ≥ÿ™ÿÆÿØŸÖ ŸÑÿ∫ÿ© ÿ™ÿ±ŸÖŸäÿ≤ (Markdown) ŸÑÿ™ŸÜÿ∏ŸäŸÖ ÿ•ÿ¨ÿßÿ®ÿßÿ™ŸÉ ÿ®Ÿàÿ∂Ÿàÿ≠ ŸàŸÜÿ∏ÿßŸÅÿ©.
ÿßŸÑŸÇŸàÿßÿπÿØ:
ÿ£ÿ¨ÿ® ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿßÿ®Ÿäÿ© ÿ®ÿ∫ÿ∂ ÿßŸÑŸÜÿ∏ÿ± ÿπŸÜ ŸÑÿ∫ÿ© ÿßŸÑÿ≥ÿ¨ŸÑ ÿ£Ÿà ÿßŸÑÿ≥ŸäÿßŸÇ.
ÿßŸÑÿ≥ŸäÿßŸÇ ŸáŸà ÿßŸÑÿ£ÿ≥ÿßÿ≥: Ÿäÿ¨ÿ® ÿ£ŸÜ ÿ™ÿπÿ™ŸÖÿØ ÿ•ÿ¨ÿßÿ®ÿßÿ™ŸÉ ÿØÿßÿ¶ŸÖŸãÿß ÿπŸÑŸâ ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑŸÖÿ™ÿßÿ≠. ŸÑÿß ÿ™ŸÇÿØŸÖ ŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÑŸÖ Ÿäÿ™ŸÖ ÿ∞ŸÉÿ±Ÿáÿß ŸÅŸä ÿßŸÑÿ≥ŸäÿßŸÇ. ŸÑÿß ÿ™ÿ∞ŸÉÿ± ÿ£ŸÜŸÉ ÿ™ÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑÿ≥ŸäÿßŸÇ ŸÑŸÑÿ•ÿ¨ÿßÿ®ÿ©. ÿ£ÿπÿØ ÿµŸäÿßÿ∫ÿ© ÿßŸÑÿ≥ŸäÿßŸÇ ŸÉŸÖÿß ÿ™ÿ±ÿßŸá ŸÖŸÜÿßÿ≥ÿ®Ÿãÿß.
ŸÑÿß ŸÑŸÑŸáŸÑŸàÿ≥ÿ©: ŸÑÿß ÿ™ÿÆÿ™ŸÑŸÇ ŸÖÿπŸÑŸàŸÖÿßÿ™. ÿ•ÿ∞ÿß ŸÑŸÖ ÿ™ÿ™ŸÖŸÉŸÜ ŸÖŸÜ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿπŸÑŸâ ÿ≥ÿ§ÿßŸÑ ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑÿ≥ŸäÿßŸÇÿå ÿßÿ∞ŸÉÿ± ÿ£ŸÜŸÉ ŸÑÿß ÿ™ŸÖŸÑŸÉ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑŸÑÿßÿ≤ŸÖÿ© ŸÑŸÑŸÖÿ≥ÿßÿπÿØÿ© ŸÅŸä Ÿáÿ∞ÿß ÿßŸÑÿßÿ≥ÿ™ŸÅÿ≥ÿßÿ± ÿßŸÑŸÖÿ≠ÿØÿØ.
ÿßŸÑÿ™ÿ≠Ÿäÿ©: ŸÑÿß ÿ™ÿ®ÿßÿØÿ± ÿ®ÿßŸÑÿ™ÿ≠Ÿäÿ© ÿ•ŸÑÿß ÿ•ÿ∞ÿß ŸÇÿßŸÖ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ ÿ®ÿ™ÿ≠Ÿäÿ™ŸÉ ÿ£ŸàŸÑÿßŸã. ÿ•ÿ∞ÿß ŸÅÿπŸÑŸàÿß ÿ∞ŸÑŸÉÿå ÿ±ÿ≠ÿ® ÿ®ŸáŸÖ ÿ®ÿ•Ÿäÿ¨ÿßÿ≤ ŸàŸÇÿØŸÖ ŸÜŸÅÿ≥ŸÉ ŸÉŸÖÿ≥ÿßÿπÿØ ÿ®ŸÑÿ™ŸàŸÜ.
ÿßŸÑÿµŸÑÿ© ÿ®ÿßŸÑŸÖŸàÿ∂Ÿàÿπ: Ÿäÿ¨ÿ® ÿ£ŸÜ ÿ™ÿ™ÿ¨ŸÜÿ® ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿπŸÑŸâ ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© ÿ∫Ÿäÿ± ÿßŸÑŸÖÿ™ÿπŸÑŸÇÿ© ÿ®ÿ®ŸÑÿ™ŸàŸÜ ÿ£Ÿà ÿÆÿØŸÖÿßÿ™Ÿáÿß.
ÿßŸÑÿ£ÿµÿßŸÑÿ©: ŸÑÿß ÿ™ŸÇÿØŸÖ ŸÜŸÅÿ≥ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ŸÖÿ±ÿ™ŸäŸÜ ÿ£ÿ®ÿØŸãÿß.
ÿßŸÑÿ•Ÿäÿ¨ÿßÿ≤: ŸÑÿß ÿ™ÿ™ÿ¨ÿßŸàÿ≤ 1000 ÿ≠ÿ±ŸÅ ŸÅŸä ÿ•ÿ¨ÿßÿ®ÿ™ŸÉ.
ÿßŸÑÿ≥ŸäÿßŸÇ:
{context}


ÿßŸÑÿ≠Ÿàÿßÿ± ÿßŸÑÿ≥ÿßÿ®ŸÇ:
{history}


ÿßŸÑÿ≥ÿ§ÿßŸÑ:
{input}
"""

arabic_prompt = PromptTemplate(input_variables=["history", "input", "context"], template=arabic_template_str)

def get_or_create_conversation_chain(user_id: str, lang: str) -> LLMChain:
    """Retrieves or creates a conversation chain for a specific user and language.

    Args:
        user_id (str): The unique identifier for the user.
        lang (str): The language code ('ar' for Arabic, 'en' for English).

    Returns:
        LLMChain: The conversation chain associated with the user and language.
    """
    if user_id not in chat_chains:
        print(f"üß† Creating new LLM chain for user '{user_id}' in '{lang}'.")
        memory = ConversationSummaryBufferMemory(
            llm=llm,
            max_token_limit=MAX_TOKEN_LIMIT,
            memory_key="history",
            input_key="input",
            return_messages=False
        )
        prompt_template = arabic_prompt if lang == "ar" else english_prompt
        chat_chains[user_id] = LLMChain(
            llm=llm,
            memory=memory,
            prompt=prompt_template,
            verbose=True
        )
    return chat_chains[user_id]


def detect_language(text):
    if detect(text) == 'ar':
        return "ar"
    else:
        return "en"


def get_search_query(question: str, lang: str) -> str:
    """Generates an optimized search query based on the input question and language.
    
    Args:
        question (str): The user's question.
        lang (str): The language code ('ar' for Arabic, 'en' for English).

    Returns:
        str: The optimized search query.
    """
    search_query = question
    if lang == "ar":
        arabic_search_prompt = f"""
        ÿµŸêÿ∫ ÿßŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ ÿßŸÑÿπÿ±ÿ®Ÿä ÿßŸÑÿ™ÿßŸÑŸä ŸÑŸäŸÉŸàŸÜ ÿßÿ≥ÿ™ÿπŸÑÿßŸÖŸãÿß Ÿàÿßÿ∂ÿ≠Ÿãÿß ŸàŸÖÿÆÿ™ÿµÿ±Ÿãÿß ŸàŸÖŸÜÿßÿ≥ÿ®Ÿãÿß ŸÑÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™.
        - ŸÑÿß ÿ™ÿ∂ŸÅ ŸÉŸÑŸÖÿßÿ™ ÿ•ÿ∂ÿßŸÅŸäÿ© ÿ∫Ÿäÿ± ÿ∂ÿ±Ÿàÿ±Ÿäÿ©.
        - ÿ•ÿ∞ÿß ŸÉÿßŸÜ Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿßÿÆÿ™ÿµÿßÿ± (ŸÖÿ´ŸÑ CEO, CFO, CTO ...) ŸÇŸÖ ÿ®ÿ™Ÿàÿ≥ŸäÿπŸá ÿ®ÿßŸÑÿßÿ≥ŸÖ ÿßŸÑŸÉÿßŸÖŸÑ ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿ•ŸÜÿ¨ŸÑŸäÿ≤Ÿäÿ©.
        
        ÿßŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ: {question}
        """
        search_query = llm.invoke(arabic_search_prompt).content.strip()
        print(f"üåç ÿßŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ ÿßŸÑÿπÿ±ÿ®Ÿä ‚Üí ÿßŸÑŸÖÿ≠ÿ≥ŸÜ: '{question}' ‚Üí '{search_query}'")
    else:
        english_search_prompt = f"""
        Rewrite the following English user query into a clear, concise query suitable for information retrieval.

        If the query contains acronyms like CEO, CTO, COO, expand them to their full forms and keep both (e.g., CEO ‚Üí CEO (Chief Executive Officer)).
        Do not expand CFO ‚Äî keep it exactly as written.
        When resolving positions disregard lines coantaing the words has media_room and awards.
        If the query explicitly refers to a role/title (e.g., chairman, CEO, CFO, president, manager, director) and is clearly tied to a person, company, or organization, add "position" at the end.
        If the query only mentions a role/title without context (no company, no person, no reference), do not add "position".
        Ensure the final query is short, direct, and information-retrieval friendly.

        Query: {question}
        """
        search_query = llm.invoke(english_search_prompt).content.strip()
        print(f"‚úç Optimized English Query: '{question}' ‚Üí '{search_query}'")
    return search_query


def classify_question_type(question: str, history: str, llm) -> str:
    """Classifies the question as either a 'follow-up' or a 'new question' based on the conversation history.
    
    Args:
        question (str): The user's question.
        history (str): The conversation history.
        llm: The language model to use for classification.
        
    Returns:
        str: 'follow-up' or 'new question'.
    
    """
    prompt = f"""
    Given the following conversation history:
    {history}
    And this new user question:
    {question}
    Determine if the new question is:
    - A 'follow-up' (refers to or depends on the previous conversation), or
    - A 'new question' (does not depend on previous context).

    Respond with only: 'follow-up' or 'new question'.
    """
    answer = llm.invoke(prompt).content.strip().lower()
    return answer

def clean_response(response: str) -> str:
    # 1. Remove <think>...</think> blocks
    response = re.sub(r"<think>.*?</think>", "", response, flags=re.DOTALL)

    # 2. Remove markdown-like bold/italic and hashtags
    response = re.sub(r"[*_#]+", "", response)

    # 3. Normalize whitespace
    response = re.sub(r"\s+", " ", response).strip()

    # 4. Optional: fix stray brackets or repeated symbols
    response = re.sub(r"[{}]+", "", response)

    return response

def rag_answer_with_memory(question: str, user_id: str, top_k: int = 7) -> str:
    """Generates an answer to the user's question using RAG with memory.

    Args:
        question (str): The user's question.
        user_id (str): The unique identifier for the user.
        top_k (int, optional): Number of top search results in qdrant. Defaults to 7.

    Returns:
        str: The generated answer.
    """
    lang = detect_language(question)
    print(f"lang detected : {lang}")
    conversation = get_or_create_conversation_chain(user_id, lang)
    history = ""
    if hasattr(conversation.memory, "buffer"):
        history = conversation.memory.buffer

    question_type = classify_question_type(question, history, llm)
    print(f"üßê Question classified as: {question_type}")

    search_query = get_search_query(question, lang)
    rag_context = get_rag_context(search_query, lang, top_k)

    rag_context = date_agent.enhance_context_with_date(rag_context, question)
    print(f"üìÑ RAG Context: {rag_context[:200]}...")

    # Conditionally set history for prompt rendering
    injected_history = history if question_type == "follow-up" else ""

    try:
        response = conversation.predict(input=question, context=rag_context, history=injected_history)
        response_clean = clean_response(response)

        return response_clean
    except Exception as e:
        print(f"‚ùå Error during generation: {e}")
        return "‚ùå An error occurred. Please try again."
    


def main():
    print(f"{'='*10}This is kai's cli let's begin testing{'='*10}\n\n")
    print("type (exit) or (quit) to terminate the session\n\n")
    status = True
    while status:
        prompt = input ("user : ")
        if "quit" in prompt or "exit" in prompt:
            status = False
        else:
            answer = rag_answer_with_memory(question= prompt , user_id= '1')
            print(f"kai's answer : {answer}")
    print(f"{'='*10}Thank you fo testing Kai, Goodbye!{'='*10}")


# uncomment this for testing in the cli
if __name__ == "__main__" :
    main()